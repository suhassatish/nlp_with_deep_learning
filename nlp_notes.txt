Word Representation Models - 2 kinds

1) Count-based distribution models. eg -
    a) SVD (Singular value decomposition)
    b) PPMI (positive pointwise mutual information) - used in UberEats SearchQuery-Restaurant matching application below.

2) Neural network-based models
    a) SGNS: Skip-gram negative sampling / Continuous Bag of Words
    b) GloVe

Paper Levy et al challenges the conventional wisdom that NN-based models are superior to count-based models.
While model design is important, hyperparameters are also KEY for achieving reasonable results.

------------
GLOVE - GLOBAL VECTORS FOR WORD REPRESENTATIONS - Its an unsupervised learning algorithm for Natural Language Processing
https://github.com/stanfordnlp/GloVe

Highlights -
------------
1) Nearest neighbors - Euclidean distance or cosine similarity b/w 2 word vectors is an effective method for measuring
semantic or linguistic similarity b/w 2 words.

2) Linear substructures - Instead of a single scalar number to represent the difference between 2 words as in something
like a levenshtein distance measure, glove computes differences between word vectors. For example - man and woman
can be similar in terms of levenshtein distance, but their word vector differences captures semantic differences in
their juxtaposition and context, depending on which they maybe diametrically opposite to each other.

Training -
------------
Trained on global word-word co-occurence matrix in a corpus. 1-time upfront cost to parse whole document to compute this
matrix. Further computations are easier since its a sparse matrix.

It is a log-bilinear model with a weighted least-squares objective.

Training objective for GloVe is to learn word vectors for words such that their dot product is equal to logarithm of
word-word co-occurrence probability.
------------

Related application - Uber Eats : matching search query to restaurant orders
https://eng.uber.com/uber-eats-query-understanding/

1) Treat an entire search query phrase of many words as 1 *word*.
2) If 2 queries lead to an order from the same restaurant, they share the same *context*.

3) Given 2 queries q1 and q2, they compute the point-wise mutual information (PMI) between them as follows -


pmi(q1, q2) = log [ p(q1, q2) / ( p(q1)^a * p(q2)^b ) ]
    where p(q1) is the marginal distribution of q1 defined as ∑_q2▒〖p(q1,q2)〗
P(q1, q2) can be approximated as the co-occurrence frequency of query q_1 and q_2 within the same context.
************************************************************************************************************

Negative sampling is a simplified model of Noise Contrastive Estimation (NCE). NCE guarantees approximation to softmax.
Negative sampling doesnt.

************************************************************************************************************

DEPENDENCY PARSING LECTURE - BUILDING A TREEBANK vs BUILDING A GRAMMAR

DISADVANTAGE of manually annotated tree bank for sentences -
1) To get started off with, its a lot slower and less useful than building a grammar.

ADVANTAGES -
1) Reusability of the labor
    Many part-of-speech taggers, etc can be built on it. Valuable resource for linguistics.

2) Broad coverage, not just a few intuitions
3) Frequencies and distributional information
4) A way to evaluate systems
------------

Phrase-structure-grammar is also known as context-free-grammar (CFG).

appos - a positional phrase

Prepositions dont have any dependents. `by` is a case marker of Brownback.

Arrow direction from HEAD to DEPENDENT.

Constraint:
Only 1 word can be a dependent of ROOT.

------------

4 common methods of DEPENDENCY PARSING -
1) Dynamic programing - O(N^3) algorithm with heads at the ends rather than in the middle.
2) Graph algorithms - create minimum spanning tree for a sentence.
3) Constraint satisfaction - edges are eliminated that dont satisfy hard constraints.
4) used in the assignment - transition-based-parsing or deterministic-dependency-parsing: Greedy choice of attachments
guided by good ML classifiers. Linear time algorithm. Very scalable. Shift-reduce parser

-----------
Pre-processing: pass the sentences 1st through a part-of-speech tagger. Then through a shift-reduce parser that attaches
dependency arrows.

HOW IS ACCURACY MEASURED of SHIFT-REDUCE PARSER?

Accuracy = # correct deps / # of deps

1) 1st look only at dependency arrow directions. This is called Unlabled Accuracy Score (UAS)
2) Or look at direction of arrow and part of speech. This is Labled Accuracy Score (LAS)
-------------
Some part-of-speech tagging terminology:

NP = noun phrase
NNS = plural noun
NN = singular noun
PP = prepositional phrase
VP = verbal phrase
NUM = numerical modifier
AMOD = adjective modifier

POS tags and dependency labels are also represented as d-dimensional vectors. Smaller discrete sets also exhibit
many semantical similarities. eg - NNS should be close to NN. NUM should be close to AMOD.

************************************************************************************************************

LECTURE8 - RNNs AND LANGUAGE MODELS -

PROBLEM EXAMPLE APPLICATION: Type-ahead search. To predict the next word for example, when the search box contains
"students opened their _____", to predict word w_j from the vocabulary given the 1st 3 words in the search query.

P(w_j | students opened their) = count(students opened their w_j) / count(students opened their)

2 problems can arise here due to sparsity of matching n-grams in the corpus:
    a) Numerator can be 0. Partial solution: SMOOTHING. ie, Add small δ to count for every w_j ε V.
    b) Denominator can be 0. Partial solution: BACKOFF. ie, Just condition on "opened their" if "students opened their"
        never appears in corpus.
------------
NEURAL LANGUAGE MODEL (fixed window) advantages over n-gram-
1) No sparsity problem
2) Model size is O(n), not O(exp(n))

DISADVANTAGES -
1) Each x(i) uses different rows of W. We dont share weights across windows.
2) Fixed Window is often too small. Increasing window size increases the size of each hidden layer.
------------

RNN-model - model visual from Lecture 8, slide 26
RNNs apply the same weight W_h at each step.

ADVANTAGES -
1) Can process any length input
2) Model size doesnt increase for longer length input
3) Computation for step t can, in theory, use information from many steps back
4) Weights are shared across time steps => representations are shared

DISADVANTAGES -
1) Recurrent computation is slow
2) In practice, difficult to access info from many steps back
------------

The gradient wrt a repeated weight in the matrix W at time t is the sum of the gradient wrt each time it appears.

Traditional EVALUATION METRIC for RNN-language models is PERPLEXITY, ie inverse probability of dataset, normalized
by number of words.

PP = Π_(t = 1 to T) 1/ [Σ_(j = 1 to |V|) (yj(t) yj_hat(t))]^(1/T)

Lower is better. Minimizing loss is equivalent to minimizing perplexity.
------------

RNN applications - speech to text, machine translation, text summarization,part-of-speech tagging,
named entity recognition, text sentiment classification. (These are all called conditional language models)

For sentence classification, we need sentence encoding, an extension of word embeddings. For this, we use the final
state of a sentence RNN with word embeddings. (slide 49). Whats better than that is teh element-wise max or mean of all
hidden states.
-------------
Microsoft Research open data for NLP applications - https://msropendata.com/
--------------
RNN notes for language modeling-
1) Loss for a single sequence = avg of cross-entropy loss of all words in a sequence

2) When reporting performance of a language model, metric generally used is PERPLEXITY, which is the inverse probability
    of correct word.
    Cross-entropy = log(perplexity)
3) Minimizing the geometric mean of perplexity is equivalent to minimizing the arithmetic mean of cross-entropy across
    the training set.

4) Heirarchical softmax computation at the output layer of an RNN is the slowest, most computationally expensive step.
    With time complexity O(D_h * |V|). Asuming |V| the #words in vocabulary >> D_h , the number of hidden units per
    layer.

    It can be sped-up to a certain extent with negative sampling, although that only improves training time and not
    testing time.
************************************************************************************************************











